{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYhILCaL/3cRciZlyowbhU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/introduction-to-ml-course/blob/main/Day3/Neural_Nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks and Deep Learning\n",
        "\n",
        "Â© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "Author: Axel Sirota"
      ],
      "metadata": {
        "id": "GS7aCs1do2Y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Networks"
      ],
      "metadata": {
        "id": "gSeET5MmpCoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network is an algorithm that goes through a sequence of steps  performing Linear and Non-Linear Algebra operations, resulting in a high capacity algorithm to perform, at first, classification. We can later tune them for other uses but the main idea here will be classification.\n",
        "\n",
        "<img src=\"https://www.dropbox.com/scl/fi/hkcsitf300tmvcqq4vnxw/neural.jpg?rlkey=x75rh7cdx73d9vfztlv2ed2hq&raw=1\"  align=\"center\"/>"
      ],
      "metadata": {
        "id": "6oe9RwbDq4QV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematically, is quite simple: Each circle, or neuron, performs the following operation:\n",
        "\n",
        "$$\n",
        "z_{i+1} = f(x_i*W_{i}^{k} + b_k)\n",
        "$$\n",
        "\n",
        "Let's dissect this formula. $x_i$ refers to the entry ith of the input Tensor X. The important part is that $w_{i}^{k}$ which is the weight for the dimension i and neuron k. Overall then if we count all neurons we have a matrix multiplication of the tensor $X$ with the weights $W$ and we have a term $b$ which are the biases and normally is set to 0.\n",
        "\n",
        "The process called training is update the weights $W$ of each layer to make the loss minimum.\n",
        "\n",
        "<img src=\"https://www.dropbox.com/scl/fi/1dluzgxbb3bqiqz4fiwsi/training.jpg?rlkey=m9sdn15i8jxzutw5vr6facvey&raw=1\"  align=\"center\"/>"
      ],
      "metadata": {
        "id": "09DxPrVduuK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data"
      ],
      "metadata": {
        "id": "65wjnqnNpIab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use one of the public datasets already parsed by Tensorflow, the IMDB one."
      ],
      "metadata": {
        "id": "9V5X9-tWv4J9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVB9pUGET0PO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Lambda, Embedding, Dropout\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.nn import leaky_relu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_features=15000\n",
        "epochs = 25\n",
        "batch_size = 256\n",
        "embedding_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")"
      ],
      "metadata": {
        "id": "jHVQN5ERUJpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to ensure our input tensor is square, so we need to calculate it's width."
      ],
      "metadata": {
        "id": "jD1SuAX_wE8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_maximum_review_length(X):\n",
        "    maximum = 0\n",
        "    for tokenized_review in X:\n",
        "        candidate = len(tokenized_review)\n",
        "        if candidate > maximum:\n",
        "            maximum = candidate\n",
        "    return maximum\n",
        "\n",
        "\n",
        "maxlen = max(get_maximum_review_length(x_train), get_maximum_review_length(x_val))"
      ],
      "metadata": {
        "id": "aON9ukOXUXTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen, padding='post')"
      ],
      "metadata": {
        "id": "-jaiAUPiUOAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "Wn6-85K0W_U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, x_val.shape, x_test.shape"
      ],
      "metadata": {
        "id": "AQz2TBFlVupr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice all our datasets have the same width, so we can input them into Tensorflow. Notice most entries will have tons of 0's and that is OK since we are goin to use a llayer called Embedding which understands 0s are pads"
      ],
      "metadata": {
        "id": "DtzlcrmOwQa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train[0]"
      ],
      "metadata": {
        "id": "FVB-_429e4gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "m3Y5zDTypb3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model will be very simple. An embedding layer and then a lot of Dense layers, which are these fully connected layers we learned before. An important aspect is the Lambda there, **can you guess why is it there?**"
      ],
      "metadata": {
        "id": "Nq3E7buewnNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Dense(50, activation=leaky_relu))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1)))\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dropout(rate=0.15))\n",
        "model.add(Dense(1))"
      ],
      "metadata": {
        "id": "SHJ2xh3kV6h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As our model does not use an activation function at the end, we must set **from_logits=True**"
      ],
      "metadata": {
        "id": "xKHZU37Tw12l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics='accuracy')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "N1eDrfaiWlpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.01, mode='max')\n",
        "history = model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), workers=5, callbacks=[callback])\n"
      ],
      "metadata": {
        "id": "usHFrNuvWuRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "oRvkf8e-X0CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "8S_AZgJdZ_x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "5CvzMPlcW17Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did good! **How would you do it to evaluate the model?** This means write a review, convert it to the corresponding Tensor and use the `predict` method to get the prediction if it's positive or not."
      ],
      "metadata": {
        "id": "UE7IV7n1w-fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now you do it\n",
        "<img src=\"https://www.dropbox.com/scl/fi/s9kv1dytq4qzr8g19y3r0/hands_on.jpg?rlkey=yz8kq22sfdgc7lsgmm1e0fksr&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>"
      ],
      "metadata": {
        "id": "Lh8mrBFoxGuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Do the exercise above on evaluating our model\n",
        "\n",
        "2) Use the Reuters news dataset from Keras with `keras.datasets.reuters.load_data()` and try to replicate what we did, test a new model and see what accuracy you get!\n",
        "\n",
        "Hint: You will find a surprise in the middle but you know how to handle it!\n",
        "Hint2: You may want to use the `CategoricalCrossEntropy` loss."
      ],
      "metadata": {
        "id": "DKaRKOHz04e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling complex datasets"
      ],
      "metadata": {
        "id": "884_iOJKiljC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we will do as above, but instead of using a processed dataset, which are rare, we will use a free text dataset of news headlines and their categories and we will predict the category."
      ],
      "metadata": {
        "id": "ZrDBMBLJ1_ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data"
      ],
      "metadata": {
        "id": "BXfbMr52jwqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'gensim==4.2.0' swifter"
      ],
      "metadata": {
        "id": "UkpXdckVipHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import warnings\n",
        "import nltk\n",
        "import swifter\n",
        "import gensim\n",
        "from keras.initializers import Constant\n",
        "\n",
        "embedding_dim = 300\n",
        "epochs=100\n",
        "batch_size = 250\n",
        "corpus_size=25000\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  K.set_session(sess)\n",
        "\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qLRHRyxXi0Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f news.csv ]; then\n",
        "  wget -O news.csv https://www.dropbox.com/s/352x7xzivf60zgc/news.csv?dl=0\n",
        "fi\n"
      ],
      "metadata": {
        "id": "TfaxFVUzjAPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash get_data.sh"
      ],
      "metadata": {
        "id": "9aRe1ibNjFJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './news.csv'\n",
        "news_pre = pd.read_csv(path, header=0).sample(n=corpus_size).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "0s_09p97jJCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_pre.head()"
      ],
      "metadata": {
        "id": "wbFbA-z92Nv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, this dataset is of text, not numbers so we need to do that mapping ourselves and be diligent on it. The first step in NLP is always to preprocess the text into tokens, in this case words"
      ],
      "metadata": {
        "id": "eTO4lbA92Va9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, should_join=True):\n",
        "    # Here you can add more magic\n",
        "    if should_join:\n",
        "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
        "    else:\n",
        "      return gensim.utils.simple_preprocess(text)"
      ],
      "metadata": {
        "id": "XrUkUgiPjZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use swifter since it is very useful to use multiprocessing on Pandas apply."
      ],
      "metadata": {
        "id": "riZanb__3ULy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = news_pre.title.swifter.apply(preprocess_text)"
      ],
      "metadata": {
        "id": "lF3x_w5QjlYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a word2vec model and the initialization Tensor"
      ],
      "metadata": {
        "id": "anPl7Dr5jzFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we said what we need is to create a Tensor such that for every sentence in a batch, for every word in that sentence, we get an ID representing that word. This will be a rectanguular tensor (because we padded) and that will be the input to the Embedding layer to later learn, for each word and sentence, the best 50 dimensional representation of the word"
      ],
      "metadata": {
        "id": "S840YHiU44Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = 'news.csv'\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield preprocess_text(line, should_join=False)\n",
        "\n",
        "import gensim.models\n",
        "\n",
        "sentences = MyCorpus()\n",
        "word2vec = gensim.models.Word2Vec(sentences=sentences, vector_size=embedding_dim)\n",
        "word2vec_model = word2vec.wv"
      ],
      "metadata": {
        "id": "MwHvZdaDj181"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! gensim is super util to create this mapping from word to index in a fast way."
      ],
      "metadata": {
        "id": "xMKU6WUN5M--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = tf.constant(word2vec_model.vectors)    # -> This goes into the Embedding layer and we will freeze it\n",
        "vocab_size = len(word2vec_model.index_to_key)"
      ],
      "metadata": {
        "id": "ba50Ep_9j4yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights.shape\n"
      ],
      "metadata": {
        "id": "SL8YfXBekCad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you check the shape it gives you for everyone of the 12342 words it has seen a 300 dimensional (in this case) representation"
      ],
      "metadata": {
        "id": "r1BnzOFN5Uv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_preprocessed = pd.DataFrame()\n",
        "news_preprocessed['label'] = news_pre.category.map({'Business': 0, 'Sports': 1, 'Sci/Tech': 2, 'World': 3})\n",
        "news_preprocessed['title'] = news\n",
        "news_preprocessed"
      ],
      "metadata": {
        "id": "cvKAORa-kC1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_maximum_review_length(df):\n",
        "    maximum = 0\n",
        "    for ix, row in df.iterrows():\n",
        "        candidate = len(preprocess_text(row.title, should_join=False))\n",
        "        if candidate > maximum:\n",
        "            maximum = candidate\n",
        "    return maximum"
      ],
      "metadata": {
        "id": "-QEIgmYlkdPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maximum = get_maximum_review_length(news_preprocessed)\n",
        "maximum"
      ],
      "metadata": {
        "id": "3tWg46HGkH_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we do what we said above. Iterate through the news df and for every word, if it exists in the word2vec model, put into X for that review and that word the index of the embedding (check index_to_key)\n"
      ],
      "metadata": {
        "id": "2E8JtlbFk01I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(news_preprocessed), maximum))\n",
        "for index, row in news_preprocessed.iterrows():\n",
        "  ix = 0\n",
        "  for word in preprocess_text(row.title, should_join=False):\n",
        "    if word in word2vec_model.key_to_index:    # If the word exists in the word2vec embedding\n",
        "      representation = word2vec_model.key_to_index[word]    # use the index\n",
        "    else:\n",
        "      representation = 0    # otherwise put a 0\n",
        "    X[index, ix] = representation\n",
        "    ix+= 1\n",
        "y = news_preprocessed.label"
      ],
      "metadata": {
        "id": "7JrqH7HRksF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "id": "80FMAJErlHJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = tf.constant(X_train)\n",
        "X_test = tf.constant(X_test)\n",
        "y_train = tf.one_hot(tf.constant(y_train), 4)  # 4 Categories\n",
        "y_test = tf.one_hot(tf.constant(y_test), 4)    # 4 Categories"
      ],
      "metadata": {
        "id": "o9q4xLT7lIc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "L4rad8GjmbpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=weights.shape[0], output_dim=embedding_dim, input_length=maximum, embeddings_initializer=Constant(weights), trainable=True))\n",
        "model.add(Dense(100, activation=leaky_relu))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(None, embedding_dim,)))\n",
        "model.add(Dense(50, activation=leaky_relu))\n",
        "model.add(Dense(4))"
      ],
      "metadata": {
        "id": "b8G0Y46Dld0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice we pass to the Embedding the weights and set al the other parameters easily. Next we compile the model but as we use many classes we must use **CategoricalCrossEntropy** as you have seen in the exercise, and we set **from_logits=True**"
      ],
      "metadata": {
        "id": "ORYvSxtd5w32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wCjPUlamloNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.01, mode='max')\n",
        "history = model.fit(x=X_train, y=y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), workers=5, callbacks=[callback])\n"
      ],
      "metadata": {
        "id": "uDgNQ0F6lr2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function for plotting loss\n",
        "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,ylim)\n",
        "    plt.plot(train_metric,color='blue',label=metric_name)\n",
        "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
        "    plt.legend(loc=\"upper right\")"
      ],
      "metadata": {
        "id": "zr2Vk5AIlwMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(history.history['loss'], history.history['val_loss'], \"Loss\", \"Loss\", ylim=10.0)\n"
      ],
      "metadata": {
        "id": "euhVd7-Nlwnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(history.history['accuracy'], history.history['val_accuracy'], \"accuracy\", \"accuracy\", ylim=1.0)\n"
      ],
      "metadata": {
        "id": "ahbm7d4FoXAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "5ylmrMQvmepj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = np.zeros((2, maximum))\n",
        "for index, row in enumerate(['supercomputer will put workers jobless soon', 'patriots goes winning super bowl']):\n",
        "    ix = 0\n",
        "    for word in preprocess_text(row, should_join=False):\n",
        "        if word not in word2vec_model:\n",
        "            representation = 0\n",
        "        else:\n",
        "            representation = word2vec_model.index_to_key.index(word)\n",
        "        x_val[index, ix] = representation\n",
        "        ix += 1\n",
        "y_val = tf.one_hot([0,1], depth=4)"
      ],
      "metadata": {
        "id": "a_0bo5yylx_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val"
      ],
      "metadata": {
        "id": "5Hujm8yvl846"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val"
      ],
      "metadata": {
        "id": "PESZqCZfl9cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x_val)"
      ],
      "metadata": {
        "id": "YM0SBFqgl-FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "f3L_HLtBl_H0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}